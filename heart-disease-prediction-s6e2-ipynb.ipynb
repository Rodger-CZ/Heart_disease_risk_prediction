{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":125192,"databundleVersionId":15408205,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Predicting Heart Disease â€” End-to-End ML Pipeline\n\nThis notebook presents a complete machine learning workflow for the Kaggle Playground Series S6E2 competition.\n\n## Workflow\n\nData inspection and quality checks\n\nRobust preprocessing using ColumnTransformer\n\nStratified cross-validation using ROC AUC\n\nBaseline model: Logistic Regression\n\nImproved model: HistGradientBoostingClassifier\n\nModel ensembling (probability averaging)\n\nKaggle submission generation\n\n## Results\n\nLogistic Regression CV AUC: ~0.950\n\nHistGradientBoosting CV AUC: ~0.955\n\nPublic Leaderboard AUC: 0.95284\n\nThe close alignment between cross-validation and leaderboard scores indicates a stable and well-validated modeling approach.\n\nThis notebook demonstrates practical tabular machine learning skills, proper validation strategy, and competition workflow.","metadata":{}},{"cell_type":"markdown","source":"# Predicting Heart Disease (Kaggle Playground Series S6E2)\n\n## Project Overview\nThis notebook builds a complete, reproducible machine learning pipeline for predicting\nthe probability of heart disease using structured tabular data. The workflow covers:\ndata inspection, cleaning, exploratory analysis, robust preprocessing, model training\nwith cross-validation, and Kaggle submission generation.\n\n## Evaluation\nSubmissions are evaluated using ROC AUC (higher is better).\n\n## Tools\nPython, pandas, numpy, matplotlib/seaborn, scikit-learn\n(Optionally: CatBoost/LightGBM)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:50.818419Z","iopub.execute_input":"2026-02-07T23:17:50.818747Z","iopub.status.idle":"2026-02-07T23:17:51.180290Z","shell.execute_reply.started":"2026-02-07T23:17:50.818721Z","shell.execute_reply":"2026-02-07T23:17:51.179318Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s6e2/sample_submission.csv\n/kaggle/input/playground-series-s6e2/train.csv\n/kaggle/input/playground-series-s6e2/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# WE start by importing the necessary Packages and Libraries\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\n\nplt.style.use(\"seaborn-v0_8\")\nRANDOM_STATE = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:51.181715Z","iopub.execute_input":"2026-02-07T23:17:51.182271Z","iopub.status.idle":"2026-02-07T23:17:52.772692Z","shell.execute_reply.started":"2026-02-07T23:17:51.182240Z","shell.execute_reply":"2026-02-07T23:17:52.771631Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# We load the data into our notebook\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s6e2/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/playground-series-s6e2/test.csv\")\nsub   = pd.read_csv(\"/kaggle/input/playground-series-s6e2/sample_submission.csv\")\n\n# We inspect the number of the fields and records in each dataset \ntrain.shape, test.shape, sub.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:52.773966Z","iopub.execute_input":"2026-02-07T23:17:52.774512Z","iopub.status.idle":"2026-02-07T23:17:54.364397Z","shell.execute_reply.started":"2026-02-07T23:17:52.774470Z","shell.execute_reply":"2026-02-07T23:17:54.363634Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"((630000, 15), (270000, 14), (270000, 2))"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Identifying the Target columns \nTARGET = list(set(train.columns) - set(test.columns))[0]\nTARGET","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:54.366312Z","iopub.execute_input":"2026-02-07T23:17:54.366572Z","iopub.status.idle":"2026-02-07T23:17:54.373111Z","shell.execute_reply.started":"2026-02-07T23:17:54.366550Z","shell.execute_reply":"2026-02-07T23:17:54.372318Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'Heart Disease'"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"Our Target columns is Heart Disease","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns=[TARGET])\ny = train[TARGET]\n\nX.head(), y.value_counts(normalize=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:54.374212Z","iopub.execute_input":"2026-02-07T23:17:54.374556Z","iopub.status.idle":"2026-02-07T23:17:54.493860Z","shell.execute_reply.started":"2026-02-07T23:17:54.374516Z","shell.execute_reply":"2026-02-07T23:17:54.493030Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(   id  Age  Sex  Chest pain type   BP  Cholesterol  FBS over 120  EKG results  \\\n 0   0   58    1                4  152          239             0            0   \n 1   1   52    1                1  125          325             0            2   \n 2   2   56    0                2  160          188             0            2   \n 3   3   44    0                3  134          229             0            2   \n 4   4   58    1                4  140          234             0            2   \n \n    Max HR  Exercise angina  ST depression  Slope of ST  \\\n 0     158                1            3.6            2   \n 1     171                0            0.0            1   \n 2     151                0            0.0            1   \n 3     150                0            1.0            2   \n 4     125                1            3.8            2   \n \n    Number of vessels fluro  Thallium  \n 0                        2         7  \n 1                        0         3  \n 2                        0         3  \n 3                        0         3  \n 4                        3         3  ,\n Heart Disease\n Absence     0.55166\n Presence    0.44834\n Name: proportion, dtype: float64)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Feature engineering and encoding\n# We separate the numerical eatures from the categorical features\nnum_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_features = X.select_dtypes(include=[\"object\"]).columns.tolist()\n\nnumeric_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, num_features),\n        (\"cat\", categorical_transformer, cat_features)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:54.494847Z","iopub.execute_input":"2026-02-07T23:17:54.495489Z","iopub.status.idle":"2026-02-07T23:17:54.533181Z","shell.execute_reply.started":"2026-02-07T23:17:54.495462Z","shell.execute_reply":"2026-02-07T23:17:54.532097Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# We proceed to logistic regression to generate the model\n# We will transform and fit our model in this cell\nmodel = LogisticRegression(max_iter=5000, solver=\"lbfgs\")\n\npipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", model)\n])\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nauc_scores = []\n\nfor tr_idx, va_idx in skf.split(X, y):\n    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n    pipeline.fit(X_tr, y_tr)\n    proba = pipeline.predict_proba(X_va)[:, 1]\n    auc_scores.append(roc_auc_score(y_va, proba))\n\nprint(\"Baseline CV AUC mean:\", np.mean(auc_scores))\nprint(\"Baseline CV AUC std :\", np.std(auc_scores))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:17:54.534264Z","iopub.execute_input":"2026-02-07T23:17:54.535247Z","iopub.status.idle":"2026-02-07T23:18:10.792066Z","shell.execute_reply.started":"2026-02-07T23:17:54.535218Z","shell.execute_reply":"2026-02-07T23:18:10.791214Z"}},"outputs":[{"name":"stdout","text":"Baseline CV AUC mean: 0.950489503531894\nBaseline CV AUC std : 0.0003377262532574926\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Note: Logistic Regression was fitted with standardized numeric features to improve\noptimization stability and avoid convergence warnings when combining scaled numeric\nfeatures with one-hot encoded categorical features.","metadata":{}},{"cell_type":"markdown","source":"## 4. Improved Model: Gradient Boosting\n\nTo capture non-linear relationships between features, a gradient boosting\nclassifier was trained and evaluated using the same stratified cross-validation\nstrategy.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\n\nhgb_model = HistGradientBoostingClassifier(\n    learning_rate=0.05,\n    max_depth=8,\n    max_iter=500,\n    random_state=RANDOM_STATE\n)\n\nhgb_pipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"model\", hgb_model)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:18:10.793185Z","iopub.execute_input":"2026-02-07T23:18:10.793587Z","iopub.status.idle":"2026-02-07T23:18:11.075473Z","shell.execute_reply.started":"2026-02-07T23:18:10.793543Z","shell.execute_reply":"2026-02-07T23:18:11.074551Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\nauc_scores_hgb = []\n\nfor tr_idx, va_idx in skf.split(X, y):\n    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n    hgb_pipeline.fit(X_tr, y_tr)\n    proba = hgb_pipeline.predict_proba(X_va)[:, 1]\n    auc_scores_hgb.append(roc_auc_score(y_va, proba))\n\nprint(\"HGB CV AUC mean:\", np.mean(auc_scores_hgb))\nprint(\"HGB CV AUC std :\", np.std(auc_scores_hgb))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:18:11.076760Z","iopub.execute_input":"2026-02-07T23:18:11.077708Z","iopub.status.idle":"2026-02-07T23:19:48.688862Z","shell.execute_reply.started":"2026-02-07T23:18:11.077609Z","shell.execute_reply":"2026-02-07T23:19:48.687894Z"}},"outputs":[{"name":"stdout","text":"HGB CV AUC mean: 0.9549893109907034\nHGB CV AUC std : 0.0004228408493996135\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Model Comparison\n\n| Model | CV AUC |\n|------|-------|\n| Logistic Regression (Baseline) | ~0.9505 |\n| HistGradientBoostingClassifier | ~0.9550 |\n\nThe gradient boosting model provides improved performance by modeling\nnon-linear feature interactions while maintaining stable cross-validation\nresults.","metadata":{}},{"cell_type":"code","source":"hgb_pipeline.fit(X, y)\n\ntest_proba = hgb_pipeline.predict_proba(test)[:, 1]\n\nsubmission = sub.copy()\nsubmission[TARGET] = test_proba\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:19:48.691184Z","iopub.execute_input":"2026-02-07T23:19:48.691558Z","iopub.status.idle":"2026-02-07T23:20:13.979834Z","shell.execute_reply.started":"2026-02-07T23:19:48.691531Z","shell.execute_reply":"2026-02-07T23:20:13.979086Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"       id  Heart Disease\n0  630000       0.937859\n1  630001       0.008232\n2  630002       0.983614\n3  630003       0.005502\n4  630004       0.203191","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Heart Disease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>630000</td>\n      <td>0.937859</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>630001</td>\n      <td>0.008232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>630002</td>\n      <td>0.983614</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>630003</td>\n      <td>0.005502</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>630004</td>\n      <td>0.203191</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)\nprint(\"submission.csv saved with shape:\", submission.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:20:13.981126Z","iopub.execute_input":"2026-02-07T23:20:13.981855Z","iopub.status.idle":"2026-02-07T23:20:14.593351Z","shell.execute_reply.started":"2026-02-07T23:20:13.981828Z","shell.execute_reply":"2026-02-07T23:20:14.592504Z"}},"outputs":[{"name":"stdout","text":"submission.csv saved with shape: (270000, 2)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"assert submission.shape[0] == test.shape[0]\nassert list(submission.columns) == list(sub.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:20:14.594396Z","iopub.execute_input":"2026-02-07T23:20:14.594643Z","iopub.status.idle":"2026-02-07T23:20:14.600461Z","shell.execute_reply.started":"2026-02-07T23:20:14.594622Z","shell.execute_reply":"2026-02-07T23:20:14.599384Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Fit baseline\npipeline.fit(X, y)\nlr_test_proba = pipeline.predict_proba(test)[:, 1]\n\n# Fit HGB\nhgb_pipeline.fit(X, y)\nhgb_test_proba = hgb_pipeline.predict_proba(test)[:, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:20:14.602006Z","iopub.execute_input":"2026-02-07T23:20:14.602382Z","iopub.status.idle":"2026-02-07T23:20:43.605816Z","shell.execute_reply.started":"2026-02-07T23:20:14.602357Z","shell.execute_reply":"2026-02-07T23:20:43.604642Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"ensemble_proba = (lr_test_proba + hgb_test_proba) / 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:20:43.607164Z","iopub.execute_input":"2026-02-07T23:20:43.607540Z","iopub.status.idle":"2026-02-07T23:20:43.612387Z","shell.execute_reply.started":"2026-02-07T23:20:43.607512Z","shell.execute_reply":"2026-02-07T23:20:43.611452Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"submission_ens = sub.copy()\nsubmission_ens[TARGET] = ensemble_proba\n\nsubmission_ens.to_csv(\"submission_ensemble.csv\", index=False)\nprint(\"submission_ensemble.csv saved\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T23:20:43.613725Z","iopub.execute_input":"2026-02-07T23:20:43.614488Z","iopub.status.idle":"2026-02-07T23:20:44.227090Z","shell.execute_reply.started":"2026-02-07T23:20:43.614460Z","shell.execute_reply":"2026-02-07T23:20:44.226124Z"}},"outputs":[{"name":"stdout","text":"submission_ensemble.csv saved\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"### Model Ensembling\n\nTo improve prediction stability, probabilities from Logistic Regression and\nHistGradientBoosting models were averaged. Ensembling combines linear and\nnon-linear model strengths and often improves generalization performance.","metadata":{}}]}